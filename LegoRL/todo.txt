TODO:
- What about train-eval modes?
- V(s_0) plotting
- Visualize Q(s, a) for Q-learning
- graphviz scheme visualisation?
- personal env in each runner? Several runners test.
- play test games each k-th frame and save animations to separate folder
- in parallel environments, draw several reward lines?
- alternative plots smoothing?
- better interface for recording
- q recording
- cateogircal Q does not work with multidimensional batch_shapes

- optimization (?): vectorized work with SumTree

- bug (?): target network loading might happen before q_net loading
- bug: priorities are equal to loss. In DQN it is square root of loss.